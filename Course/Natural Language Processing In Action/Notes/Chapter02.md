## 第2章 构建自己的词汇表 —— 分词



### 导言

#### 从文档中检索词条

1. 需要一些字符串处理方法。处理时需要把标点符号与词分开，还需要将 we'll 这样的缩写词还原成原始词。
2. 使用正则表达式将意义相似的词合并在一起，这个过程称为**词干还原（stemming）**。
3. 然后，就可以将文档表示成**词袋（BOW，bag-of-words）**向量。





#### n-gram

可以提取出连续2个、3个、4个甚至5个词条组成的词对、三元组、四元组和五元组。这些语言单位称为n-gram（n元）。连续两个词称为2-gram（bigram），连续3个词称为3-gram（trigram），连续4个词称为4-gram，其余以此类推。利用n-gram可以让机器不仅认识“ice”和“cream”，也认识它们构成的2-gram“ice cream”。





### 词干还原

#### 词干还原

所谓词干还原，指的是将某个词的不同屈折变化形式统统“打包”到同一个“桶”或者类别中。



#### 利用分词器构建词汇表

#### 分词 tokenization

在NLP中，分词（tokenization，也称切词）是一种特殊的文档切分（segmentation）过程。而文档切分能够将文本拆分成更小的文本块或片段，其中含有更集中的信息内容。文档切分可以是将文档分成段落，将段落分成句子，将句子分成短语，或将短语分成词条（通常是词）和标点符号。本章主要关注将文本分割成词条的过程，这个过程称为分词。



对于NLP的基础构建模块，计算机语言编译器中存在一些与它们等同的模块：

| NLP构建模块          | 计算机语言编译器     |
| -------------------- | -------------------- |
| 分词器               | 扫描器（词法分析器） |
| 词汇表               | 词库                 |
| 分析器               | 编译器               |
| 词条/词项/词或n-gram | 标识符或终结符       |





#### 独热向量 one-hot vector

